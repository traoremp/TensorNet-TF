/content/TensorNet-TF/experiments/cifar-10/FC-Tensorizing-Neural-Networks/tt-layer_and_quantization
/content/TensorNet-TF/experiments/cifar-10/FC-Tensorizing-Neural-Networks/tt-layer_and_quantization
2020-05-15 04:10:54.176492: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
W0515 04:10:58.263110 140692897838976 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2020-05-15 04:10:58.367985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-15 04:10:58.407383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-15 04:10:58.408030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s
2020-05-15 04:10:58.408074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-15 04:10:58.409619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-15 04:10:58.411224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-15 04:10:58.411544: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-15 04:10:58.413029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-15 04:10:58.413840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-15 04:10:58.416828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-15 04:10:58.416932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-15 04:10:58.417514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-15 04:10:58.418091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
WARNING:tensorflow:From /content/TensorNet-TF/experiments/cifar-10/FC-Tensorizing-Neural-Networks/tt-layer_and_quantization/net.py:93: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0515 04:10:58.440323 140692897838976 deprecation.py:323] From /content/TensorNet-TF/experiments/cifar-10/FC-Tensorizing-Neural-Networks/tt-layer_and_quantization/net.py:93: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /content/TensorNet-TF/experiments/cifar-10/FC-Tensorizing-Neural-Networks/tt-layer_and_quantization/net.py:117: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0515 04:10:58.442364 140692897838976 deprecation.py:506] From /content/TensorNet-TF/experiments/cifar-10/FC-Tensorizing-Neural-Networks/tt-layer_and_quantization/net.py:117: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /content/TensorNet-TF/experiments/cifar-10/FC-Tensorizing-Neural-Networks/tt-layer_and_quantization/net.py:144: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
W0515 04:10:58.713477 140692897838976 deprecation.py:323] From /content/TensorNet-TF/experiments/cifar-10/FC-Tensorizing-Neural-Networks/tt-layer_and_quantization/net.py:144: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From /content/TensorNet-TF/experiments/cifar-10/FC-Tensorizing-Neural-Networks/tt-layer_and_quantization/net.py:149: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

W0515 04:10:58.714809 140692897838976 deprecation.py:323] From /content/TensorNet-TF/experiments/cifar-10/FC-Tensorizing-Neural-Networks/tt-layer_and_quantization/net.py:149: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

2020-05-15 04:10:59.294893: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2020-05-15 04:10:59.299356: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2000125000 Hz
2020-05-15 04:10:59.299800: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1679100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-15 04:10:59.299837: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-15 04:10:59.396860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-15 04:10:59.397540: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1678bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-05-15 04:10:59.397569: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5
2020-05-15 04:10:59.397748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-15 04:10:59.398328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s
2020-05-15 04:10:59.398372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-15 04:10:59.398412: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-15 04:10:59.398429: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-15 04:10:59.398447: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-15 04:10:59.398465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-15 04:10:59.398482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-15 04:10:59.398499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-15 04:10:59.398571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-15 04:10:59.399137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-15 04:10:59.399604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-05-15 04:10:59.399649: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-15 04:10:59.786454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-15 04:10:59.786517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-05-15 04:10:59.786531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-05-15 04:10:59.786731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-15 04:10:59.787389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-15 04:10:59.787997: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2020-05-15 04:10:59.788046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13970 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
2020-05-15 04:11:00.412614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Step 100: loss = 3.00 (0.489 sec) [204.63 data/s]
Step 200: loss = 2.64 (0.506 sec) [197.53 data/s]
Step 300: loss = 2.35 (0.519 sec) [192.57 data/s]
Step 400: loss = 2.54 (0.519 sec) [192.79 data/s]
Step 500: loss = 2.47 (0.517 sec) [193.60 data/s]
Step 600: loss = 2.28 (0.522 sec) [191.69 data/s]
Step 700: loss = 2.19 (0.516 sec) [193.84 data/s]
Step 800: loss = 2.18 (0.515 sec) [194.21 data/s]
Step 900: loss = 2.31 (0.515 sec) [194.08 data/s]
Step 1000: loss = 2.22 (0.516 sec) [193.74 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 17272  Precision @ 1: 0.3454  Loss: 1.96
Validation Data Eval:
  Num examples: 10000  Num correct: 3318  Precision @ 1: 0.3318  Loss: 2.00
Step 1100: loss = 2.02 (0.515 sec) [194.27 data/s]
Step 1200: loss = 2.14 (0.520 sec) [192.46 data/s]
Step 1300: loss = 2.09 (0.513 sec) [195.01 data/s]
Step 1400: loss = 2.07 (0.520 sec) [192.28 data/s]
Step 1500: loss = 2.13 (0.517 sec) [193.53 data/s]
Step 1600: loss = 1.94 (0.517 sec) [193.57 data/s]
Step 1700: loss = 2.02 (0.517 sec) [193.37 data/s]
Step 1800: loss = 2.13 (0.514 sec) [194.45 data/s]
Step 1900: loss = 1.96 (0.515 sec) [194.21 data/s]
Step 2000: loss = 1.96 (0.522 sec) [191.74 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18599  Precision @ 1: 0.3720  Loss: 1.88
Validation Data Eval:
  Num examples: 10000  Num correct: 3572  Precision @ 1: 0.3572  Loss: 1.92
Step 2100: loss = 2.00 (0.512 sec) [195.38 data/s]
Step 2200: loss = 1.92 (0.514 sec) [194.37 data/s]
Step 2300: loss = 2.02 (0.516 sec) [193.91 data/s]
Step 2400: loss = 2.01 (0.515 sec) [194.31 data/s]
Step 2500: loss = 1.71 (0.520 sec) [192.42 data/s]
Step 2600: loss = 1.86 (0.517 sec) [193.50 data/s]
Step 2700: loss = 2.11 (0.516 sec) [193.91 data/s]
Step 2800: loss = 1.95 (0.516 sec) [193.63 data/s]
Step 2900: loss = 2.10 (0.516 sec) [193.81 data/s]
Step 3000: loss = 2.09 (0.514 sec) [194.40 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18101  Precision @ 1: 0.3620  Loss: 1.95
Validation Data Eval:
  Num examples: 10000  Num correct: 3550  Precision @ 1: 0.3550  Loss: 1.97
Step 3100: loss = 2.13 (0.518 sec) [193.16 data/s]
Step 3200: loss = 2.10 (0.517 sec) [193.51 data/s]
Step 3300: loss = 2.13 (0.519 sec) [192.52 data/s]
Step 3400: loss = 1.91 (0.512 sec) [195.21 data/s]
Step 3500: loss = 2.06 (0.517 sec) [193.26 data/s]
Step 3600: loss = 1.97 (0.519 sec) [192.77 data/s]
Step 3700: loss = 2.08 (0.510 sec) [195.95 data/s]
Step 3800: loss = 2.19 (0.510 sec) [195.95 data/s]
Step 3900: loss = 1.95 (0.509 sec) [196.46 data/s]
Step 4000: loss = 1.96 (0.514 sec) [194.56 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18757  Precision @ 1: 0.3751  Loss: 1.88
Validation Data Eval:
  Num examples: 10000  Num correct: 3701  Precision @ 1: 0.3701  Loss: 1.90
Step 4100: loss = 1.83 (0.520 sec) [192.46 data/s]
Step 4200: loss = 1.91 (0.516 sec) [193.78 data/s]
Step 4300: loss = 2.01 (0.520 sec) [192.14 data/s]
Step 4400: loss = 2.02 (0.513 sec) [194.93 data/s]
Step 4500: loss = 1.98 (0.519 sec) [192.79 data/s]
Step 4600: loss = 1.96 (0.514 sec) [194.63 data/s]
Step 4700: loss = 2.04 (0.516 sec) [193.93 data/s]
Step 4800: loss = 1.75 (0.511 sec) [195.66 data/s]
Step 4900: loss = 1.90 (0.517 sec) [193.35 data/s]
Step 5000: loss = 2.00 (0.518 sec) [193.08 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 19294  Precision @ 1: 0.3859  Loss: 1.83
Validation Data Eval:
  Num examples: 10000  Num correct: 3796  Precision @ 1: 0.3796  Loss: 1.85
Step 5100: loss = 2.15 (0.513 sec) [195.12 data/s]
Step 5200: loss = 1.95 (0.517 sec) [193.51 data/s]
Step 5300: loss = 1.98 (0.517 sec) [193.57 data/s]
Step 5400: loss = 1.96 (0.520 sec) [192.28 data/s]
Step 5500: loss = 1.72 (0.517 sec) [193.33 data/s]
Step 5600: loss = 1.99 (0.517 sec) [193.28 data/s]
Step 5700: loss = 1.91 (0.516 sec) [193.81 data/s]
Step 5800: loss = 1.98 (0.518 sec) [193.07 data/s]
Step 5900: loss = 1.96 (0.513 sec) [194.85 data/s]
Step 6000: loss = 2.14 (0.517 sec) [193.32 data/s]
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
W0515 05:13:05.564105 140692897838976 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Training Data Eval:
  Num examples: 50000  Num correct: 19056  Precision @ 1: 0.3811  Loss: 1.84
Validation Data Eval:
  Num examples: 10000  Num correct: 3788  Precision @ 1: 0.3788  Loss: 1.86
Step 6100: loss = 2.10 (0.517 sec) [193.58 data/s]
Step 6200: loss = 1.99 (0.519 sec) [192.65 data/s]
Step 6300: loss = 2.00 (0.522 sec) [191.74 data/s]
Step 6400: loss = 2.09 (0.516 sec) [193.87 data/s]
Step 6500: loss = 2.03 (0.516 sec) [193.97 data/s]
Step 6600: loss = 2.21 (0.514 sec) [194.50 data/s]
Step 6700: loss = 2.12 (0.509 sec) [196.46 data/s]
Step 6800: loss = 1.86 (0.517 sec) [193.28 data/s]
Step 6900: loss = 1.97 (0.514 sec) [194.43 data/s]
Step 7000: loss = 2.00 (0.515 sec) [194.33 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 17908  Precision @ 1: 0.3582  Loss: 1.89
Validation Data Eval:
  Num examples: 10000  Num correct: 3520  Precision @ 1: 0.3520  Loss: 1.92
Step 7100: loss = 1.88 (0.514 sec) [194.52 data/s]
Step 7200: loss = 2.01 (0.515 sec) [194.32 data/s]
Step 7300: loss = 2.09 (0.513 sec) [195.10 data/s]
Step 7400: loss = 1.89 (0.517 sec) [193.48 data/s]
Step 7500: loss = 1.84 (0.516 sec) [193.90 data/s]
Step 7600: loss = 2.02 (0.517 sec) [193.61 data/s]
Step 7700: loss = 2.02 (0.519 sec) [192.71 data/s]
Step 7800: loss = 1.87 (0.518 sec) [193.01 data/s]
Step 7900: loss = 1.91 (0.516 sec) [193.88 data/s]
Step 8000: loss = 1.93 (0.514 sec) [194.66 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18622  Precision @ 1: 0.3724  Loss: 1.86
Validation Data Eval:
  Num examples: 10000  Num correct: 3628  Precision @ 1: 0.3628  Loss: 1.89
Step 8100: loss = 2.05 (0.514 sec) [194.66 data/s]
Step 8200: loss = 2.08 (0.516 sec) [193.85 data/s]
Step 8300: loss = 1.91 (0.519 sec) [192.81 data/s]
Step 8400: loss = 1.82 (0.508 sec) [197.03 data/s]
Step 8500: loss = 1.99 (0.509 sec) [196.38 data/s]
Step 8600: loss = 2.26 (0.520 sec) [192.43 data/s]
Step 8700: loss = 2.00 (0.510 sec) [195.96 data/s]
Step 8800: loss = 2.03 (0.509 sec) [196.40 data/s]
Step 8900: loss = 1.94 (0.514 sec) [194.65 data/s]
Step 9000: loss = 1.88 (0.518 sec) [193.15 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 17889  Precision @ 1: 0.3578  Loss: 1.95
Validation Data Eval:
  Num examples: 10000  Num correct: 3481  Precision @ 1: 0.3481  Loss: 1.96
Step 9100: loss = 2.18 (0.523 sec) [191.17 data/s]
Step 9200: loss = 1.96 (0.512 sec) [195.16 data/s]
Step 9300: loss = 1.94 (0.516 sec) [193.94 data/s]
Step 9400: loss = 1.81 (0.519 sec) [192.62 data/s]
Step 9500: loss = 2.06 (0.517 sec) [193.40 data/s]
Step 9600: loss = 1.83 (0.514 sec) [194.43 data/s]
Step 9700: loss = 1.94 (0.513 sec) [194.83 data/s]
Step 9800: loss = 1.97 (0.515 sec) [194.30 data/s]
Step 9900: loss = 1.86 (0.513 sec) [194.81 data/s]
Step 10000: loss = 1.78 (0.512 sec) [195.16 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 17850  Precision @ 1: 0.3570  Loss: 1.90
Validation Data Eval:
  Num examples: 10000  Num correct: 3470  Precision @ 1: 0.3470  Loss: 1.93
Step 10100: loss = 2.03 (0.514 sec) [194.45 data/s]
Step 10200: loss = 1.95 (0.516 sec) [193.95 data/s]
Step 10300: loss = 2.03 (0.515 sec) [194.11 data/s]
Step 10400: loss = 1.99 (0.509 sec) [196.32 data/s]
Step 10500: loss = 2.15 (0.517 sec) [193.41 data/s]
Step 10600: loss = 1.94 (0.513 sec) [195.01 data/s]
Step 10700: loss = 2.00 (0.519 sec) [192.60 data/s]
Step 10800: loss = 1.81 (0.513 sec) [195.12 data/s]
Step 10900: loss = 2.10 (0.513 sec) [194.85 data/s]
Step 11000: loss = 1.85 (0.517 sec) [193.43 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18308  Precision @ 1: 0.3662  Loss: 1.88
Validation Data Eval:
  Num examples: 10000  Num correct: 3596  Precision @ 1: 0.3596  Loss: 1.90
Step 11100: loss = 2.23 (0.517 sec) [193.33 data/s]
Step 11200: loss = 2.06 (0.512 sec) [195.20 data/s]
Step 11300: loss = 1.82 (0.514 sec) [194.42 data/s]
Step 11400: loss = 2.13 (0.516 sec) [193.77 data/s]
Step 11500: loss = 1.94 (0.515 sec) [194.33 data/s]
Step 11600: loss = 1.82 (0.515 sec) [194.16 data/s]
Step 11700: loss = 1.94 (0.517 sec) [193.48 data/s]
Step 11800: loss = 1.82 (0.516 sec) [193.88 data/s]
Step 11900: loss = 2.08 (0.514 sec) [194.71 data/s]
Step 12000: loss = 1.95 (0.509 sec) [196.40 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18063  Precision @ 1: 0.3613  Loss: 1.89
Validation Data Eval:
  Num examples: 10000  Num correct: 3552  Precision @ 1: 0.3552  Loss: 1.91
Step 12100: loss = 2.12 (0.516 sec) [193.72 data/s]
Step 12200: loss = 1.85 (0.521 sec) [191.84 data/s]
Step 12300: loss = 1.97 (0.511 sec) [195.79 data/s]
Step 12400: loss = 2.16 (0.517 sec) [193.56 data/s]
Step 12500: loss = 2.00 (0.517 sec) [193.30 data/s]
Step 12600: loss = 2.00 (0.515 sec) [194.36 data/s]
Step 12700: loss = 2.12 (0.517 sec) [193.51 data/s]
Step 12800: loss = 2.00 (0.515 sec) [194.16 data/s]
Step 12900: loss = 1.99 (0.518 sec) [193.14 data/s]
Step 13000: loss = 2.06 (0.520 sec) [192.22 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18962  Precision @ 1: 0.3792  Loss: 1.85
Validation Data Eval:
  Num examples: 10000  Num correct: 3746  Precision @ 1: 0.3746  Loss: 1.86
Step 13100: loss = 2.19 (0.515 sec) [193.99 data/s]
Step 13200: loss = 2.04 (0.511 sec) [195.68 data/s]
Step 13300: loss = 2.03 (0.521 sec) [192.03 data/s]
Step 13400: loss = 1.98 (0.516 sec) [193.87 data/s]
Step 13500: loss = 2.03 (0.514 sec) [194.38 data/s]
Step 13600: loss = 1.99 (0.514 sec) [194.62 data/s]
Step 13700: loss = 2.06 (0.516 sec) [193.68 data/s]
Step 13800: loss = 1.81 (0.515 sec) [194.03 data/s]
Step 13900: loss = 1.81 (0.511 sec) [195.53 data/s]
Step 14000: loss = 1.97 (0.510 sec) [195.96 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18958  Precision @ 1: 0.3792  Loss: 1.83
Validation Data Eval:
  Num examples: 10000  Num correct: 3697  Precision @ 1: 0.3697  Loss: 1.85
Step 14100: loss = 1.83 (0.517 sec) [193.38 data/s]
Step 14200: loss = 1.87 (0.516 sec) [193.68 data/s]
Step 14300: loss = 1.92 (0.518 sec) [193.22 data/s]
Step 14400: loss = 2.08 (0.515 sec) [194.27 data/s]
Step 14500: loss = 1.95 (0.519 sec) [192.58 data/s]
Step 14600: loss = 2.05 (0.513 sec) [195.11 data/s]
Step 14700: loss = 2.09 (0.516 sec) [193.98 data/s]
Step 14800: loss = 2.10 (0.518 sec) [192.97 data/s]
Step 14900: loss = 2.17 (0.517 sec) [193.40 data/s]
Step 15000: loss = 1.88 (0.513 sec) [194.76 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18436  Precision @ 1: 0.3687  Loss: 1.89
Validation Data Eval:
  Num examples: 10000  Num correct: 3678  Precision @ 1: 0.3678  Loss: 1.90
Step 15100: loss = 1.96 (0.519 sec) [192.86 data/s]
Step 15200: loss = 2.05 (0.515 sec) [194.24 data/s]
Step 15300: loss = 1.98 (0.515 sec) [194.19 data/s]
Step 15400: loss = 2.24 (0.521 sec) [191.92 data/s]
Step 15500: loss = 2.04 (0.517 sec) [193.55 data/s]
Step 15600: loss = 2.18 (0.514 sec) [194.71 data/s]
Step 15700: loss = 1.98 (0.518 sec) [193.21 data/s]
Step 15800: loss = 2.32 (0.517 sec) [193.31 data/s]
Step 15900: loss = 1.98 (0.514 sec) [194.57 data/s]
Step 16000: loss = 1.93 (0.515 sec) [194.00 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18583  Precision @ 1: 0.3717  Loss: 1.86
Validation Data Eval:
  Num examples: 10000  Num correct: 3687  Precision @ 1: 0.3687  Loss: 1.88
Step 16100: loss = 1.99 (0.509 sec) [196.48 data/s]
Step 16200: loss = 1.98 (0.518 sec) [193.01 data/s]
Step 16300: loss = 2.05 (0.520 sec) [192.35 data/s]
Step 16400: loss = 1.91 (0.519 sec) [192.84 data/s]
Step 16500: loss = 1.92 (0.519 sec) [192.78 data/s]
Step 16600: loss = 2.12 (0.514 sec) [194.41 data/s]
Step 16700: loss = 1.88 (0.519 sec) [192.63 data/s]
Step 16800: loss = 1.94 (0.515 sec) [194.21 data/s]
Step 16900: loss = 2.03 (0.510 sec) [196.25 data/s]
Step 17000: loss = 2.21 (0.518 sec) [193.15 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18749  Precision @ 1: 0.3750  Loss: 1.88
Validation Data Eval:
  Num examples: 10000  Num correct: 3693  Precision @ 1: 0.3693  Loss: 1.89
Step 17100: loss = 2.07 (0.515 sec) [194.02 data/s]
Step 17200: loss = 1.97 (0.510 sec) [196.03 data/s]
Step 17300: loss = 1.95 (0.512 sec) [195.31 data/s]
Step 17400: loss = 1.94 (0.517 sec) [193.28 data/s]
Step 17500: loss = 2.03 (0.516 sec) [193.96 data/s]
Step 17600: loss = 1.94 (0.511 sec) [195.56 data/s]
Step 17700: loss = 1.97 (0.514 sec) [194.60 data/s]
Step 17800: loss = 2.04 (0.515 sec) [194.17 data/s]
Step 17900: loss = 1.91 (0.518 sec) [192.87 data/s]
Step 18000: loss = 2.12 (0.517 sec) [193.54 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18601  Precision @ 1: 0.3720  Loss: 1.83
Validation Data Eval:
  Num examples: 10000  Num correct: 3620  Precision @ 1: 0.3620  Loss: 1.86
Step 18100: loss = 1.87 (0.515 sec) [194.17 data/s]
Step 18200: loss = 1.90 (0.515 sec) [194.36 data/s]
Step 18300: loss = 1.96 (0.514 sec) [194.50 data/s]
Step 18400: loss = 1.87 (0.518 sec) [192.93 data/s]
Step 18500: loss = 2.04 (0.518 sec) [193.18 data/s]
Step 18600: loss = 1.76 (0.510 sec) [196.18 data/s]
Step 18700: loss = 2.02 (0.517 sec) [193.56 data/s]
Step 18800: loss = 2.00 (0.516 sec) [193.93 data/s]
Step 18900: loss = 1.94 (0.518 sec) [193.05 data/s]
Step 19000: loss = 1.92 (0.514 sec) [194.47 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18419  Precision @ 1: 0.3684  Loss: 1.89
Validation Data Eval:
  Num examples: 10000  Num correct: 3681  Precision @ 1: 0.3681  Loss: 1.89
Step 19100: loss = 1.97 (0.514 sec) [194.70 data/s]
Step 19200: loss = 1.97 (0.515 sec) [194.05 data/s]
Step 19300: loss = 1.89 (0.515 sec) [194.12 data/s]
Step 19400: loss = 2.02 (0.516 sec) [193.98 data/s]
Step 19500: loss = 1.88 (0.511 sec) [195.58 data/s]
Step 19600: loss = 2.00 (0.515 sec) [194.07 data/s]
Step 19700: loss = 2.13 (0.519 sec) [192.69 data/s]
Step 19800: loss = 1.98 (0.517 sec) [193.52 data/s]
Step 19900: loss = 2.09 (0.516 sec) [193.61 data/s]
Step 20000: loss = 2.20 (0.512 sec) [195.39 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 19015  Precision @ 1: 0.3803  Loss: 1.84
Validation Data Eval:
  Num examples: 10000  Num correct: 3738  Precision @ 1: 0.3738  Loss: 1.87
Step 20100: loss = 2.10 (0.513 sec) [194.92 data/s]
Step 20200: loss = 2.02 (0.508 sec) [196.71 data/s]
Step 20300: loss = 2.07 (0.517 sec) [193.37 data/s]
Step 20400: loss = 1.89 (0.509 sec) [196.48 data/s]
Step 20500: loss = 1.83 (0.513 sec) [194.94 data/s]
Step 20600: loss = 2.20 (0.520 sec) [192.47 data/s]
Step 20700: loss = 1.87 (0.514 sec) [194.54 data/s]
Step 20800: loss = 2.16 (0.518 sec) [193.14 data/s]
Step 20900: loss = 1.99 (0.509 sec) [196.29 data/s]
Step 21000: loss = 1.84 (0.518 sec) [193.02 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18354  Precision @ 1: 0.3671  Loss: 1.91
Validation Data Eval:
  Num examples: 10000  Num correct: 3597  Precision @ 1: 0.3597  Loss: 1.93
Step 21100: loss = 2.23 (0.514 sec) [194.70 data/s]
Step 21200: loss = 1.95 (0.516 sec) [193.64 data/s]
Step 21300: loss = 2.02 (0.516 sec) [193.91 data/s]
Step 21400: loss = 2.07 (0.516 sec) [193.72 data/s]
Step 21500: loss = 1.99 (0.515 sec) [194.35 data/s]
Step 21600: loss = 1.93 (0.509 sec) [196.31 data/s]
Step 21700: loss = 2.10 (0.526 sec) [189.99 data/s]
Step 21800: loss = 2.05 (0.517 sec) [193.47 data/s]
Step 21900: loss = 1.94 (0.513 sec) [195.07 data/s]
Step 22000: loss = 2.04 (0.516 sec) [193.64 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18129  Precision @ 1: 0.3626  Loss: 1.93
Validation Data Eval:
  Num examples: 10000  Num correct: 3605  Precision @ 1: 0.3605  Loss: 1.93
Step 22100: loss = 2.03 (0.516 sec) [193.87 data/s]
Step 22200: loss = 1.91 (0.518 sec) [193.12 data/s]
Step 22300: loss = 1.96 (0.517 sec) [193.46 data/s]
Step 22400: loss = 2.02 (0.512 sec) [195.47 data/s]
Step 22500: loss = 2.07 (0.516 sec) [193.62 data/s]
Step 22600: loss = 1.92 (0.516 sec) [193.97 data/s]
Step 22700: loss = 2.38 (0.517 sec) [193.32 data/s]
Step 22800: loss = 1.88 (0.512 sec) [195.23 data/s]
Step 22900: loss = 2.03 (0.514 sec) [194.73 data/s]
Step 23000: loss = 2.19 (0.517 sec) [193.51 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18930  Precision @ 1: 0.3786  Loss: 1.84
Validation Data Eval:
  Num examples: 10000  Num correct: 3724  Precision @ 1: 0.3724  Loss: 1.85
Step 23100: loss = 1.92 (0.512 sec) [195.25 data/s]
Step 23200: loss = 2.02 (0.512 sec) [195.45 data/s]
Step 23300: loss = 1.97 (0.511 sec) [195.79 data/s]
Step 23400: loss = 2.09 (0.514 sec) [194.63 data/s]
Step 23500: loss = 1.88 (0.516 sec) [193.80 data/s]
Step 23600: loss = 1.83 (0.516 sec) [193.96 data/s]
Step 23700: loss = 1.92 (0.516 sec) [193.76 data/s]
Step 23800: loss = 2.00 (0.509 sec) [196.33 data/s]
Step 23900: loss = 1.97 (0.516 sec) [193.63 data/s]
Step 24000: loss = 2.01 (0.513 sec) [194.82 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 19007  Precision @ 1: 0.3801  Loss: 1.81
Validation Data Eval:
  Num examples: 10000  Num correct: 3725  Precision @ 1: 0.3725  Loss: 1.83
Step 24100: loss = 1.78 (0.512 sec) [195.16 data/s]
Step 24200: loss = 2.14 (0.511 sec) [195.63 data/s]
Step 24300: loss = 2.12 (0.510 sec) [196.25 data/s]
Step 24400: loss = 1.95 (0.508 sec) [196.98 data/s]
Step 24500: loss = 1.99 (0.522 sec) [191.59 data/s]
Step 24600: loss = 1.85 (0.516 sec) [193.98 data/s]
Step 24700: loss = 1.96 (0.512 sec) [195.33 data/s]
Step 24800: loss = 1.89 (0.513 sec) [195.10 data/s]
Step 24900: loss = 1.92 (0.517 sec) [193.58 data/s]
Step 25000: loss = 1.98 (0.514 sec) [194.53 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18287  Precision @ 1: 0.3657  Loss: 1.90
Validation Data Eval:
  Num examples: 10000  Num correct: 3625  Precision @ 1: 0.3625  Loss: 1.90
Step 25100: loss = 2.10 (0.517 sec) [193.49 data/s]
Step 25200: loss = 2.04 (0.515 sec) [194.32 data/s]
Step 25300: loss = 1.95 (0.515 sec) [194.24 data/s]
Step 25400: loss = 1.82 (0.508 sec) [196.80 data/s]
Step 25500: loss = 1.96 (0.517 sec) [193.26 data/s]
Step 25600: loss = 2.03 (0.516 sec) [193.67 data/s]
Step 25700: loss = 2.01 (0.514 sec) [194.71 data/s]
Step 25800: loss = 1.93 (0.511 sec) [195.76 data/s]
Step 25900: loss = 1.87 (0.519 sec) [192.60 data/s]
Step 26000: loss = 2.02 (0.519 sec) [192.84 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18706  Precision @ 1: 0.3741  Loss: 1.86
Validation Data Eval:
  Num examples: 10000  Num correct: 3699  Precision @ 1: 0.3699  Loss: 1.88
Step 26100: loss = 2.02 (0.515 sec) [194.32 data/s]
Step 26200: loss = 1.97 (0.514 sec) [194.37 data/s]
Step 26300: loss = 1.92 (0.516 sec) [193.74 data/s]
Step 26400: loss = 1.96 (0.520 sec) [192.25 data/s]
Step 26500: loss = 2.20 (0.512 sec) [195.18 data/s]
Step 26600: loss = 1.86 (0.511 sec) [195.87 data/s]
Step 26700: loss = 1.82 (0.517 sec) [193.35 data/s]
Step 26800: loss = 1.99 (0.516 sec) [193.82 data/s]
Step 26900: loss = 2.04 (0.512 sec) [195.13 data/s]
Step 27000: loss = 1.83 (0.517 sec) [193.29 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 19059  Precision @ 1: 0.3812  Loss: 1.85
Validation Data Eval:
  Num examples: 10000  Num correct: 3710  Precision @ 1: 0.3710  Loss: 1.88
Step 27100: loss = 2.05 (0.513 sec) [195.11 data/s]
Step 27200: loss = 1.92 (0.521 sec) [192.00 data/s]
Step 27300: loss = 1.96 (0.516 sec) [193.93 data/s]
Step 27400: loss = 1.94 (0.518 sec) [193.12 data/s]
Step 27500: loss = 2.05 (0.517 sec) [193.49 data/s]
Step 27600: loss = 1.66 (0.508 sec) [196.80 data/s]
Step 27700: loss = 1.98 (0.514 sec) [194.63 data/s]
Step 27800: loss = 1.97 (0.515 sec) [194.19 data/s]
Step 27900: loss = 1.87 (0.516 sec) [193.71 data/s]
Step 28000: loss = 1.82 (0.513 sec) [194.89 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 19289  Precision @ 1: 0.3858  Loss: 1.80
Validation Data Eval:
  Num examples: 10000  Num correct: 3813  Precision @ 1: 0.3813  Loss: 1.82
Step 28100: loss = 1.98 (0.519 sec) [192.59 data/s]
Step 28200: loss = 1.93 (0.516 sec) [193.91 data/s]
Step 28300: loss = 1.95 (0.517 sec) [193.36 data/s]
Step 28400: loss = 2.04 (0.512 sec) [195.33 data/s]
Step 28500: loss = 2.02 (0.514 sec) [194.39 data/s]
Step 28600: loss = 2.11 (0.516 sec) [193.78 data/s]
Step 28700: loss = 1.91 (0.515 sec) [194.03 data/s]
Step 28800: loss = 1.93 (0.517 sec) [193.46 data/s]
Step 28900: loss = 2.13 (0.518 sec) [193.07 data/s]
Step 29000: loss = 2.13 (0.515 sec) [194.32 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18867  Precision @ 1: 0.3773  Loss: 1.84
Validation Data Eval:
  Num examples: 10000  Num correct: 3708  Precision @ 1: 0.3708  Loss: 1.85
Step 29100: loss = 1.99 (0.513 sec) [194.97 data/s]
Step 29200: loss = 1.76 (0.515 sec) [194.18 data/s]
Step 29300: loss = 1.83 (0.511 sec) [195.62 data/s]
Step 29400: loss = 1.94 (0.512 sec) [195.35 data/s]
Step 29500: loss = 2.21 (0.517 sec) [193.39 data/s]
Step 29600: loss = 1.86 (0.517 sec) [193.38 data/s]
Step 29700: loss = 1.94 (0.511 sec) [195.86 data/s]
Step 29800: loss = 1.87 (0.520 sec) [192.31 data/s]
Step 29900: loss = 2.02 (0.515 sec) [194.28 data/s]
Step 30000: loss = 1.80 (0.518 sec) [193.17 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 19350  Precision @ 1: 0.3870  Loss: 1.82
Validation Data Eval:
  Num examples: 10000  Num correct: 3744  Precision @ 1: 0.3744  Loss: 1.84
Step 30100: loss = 2.00 (0.514 sec) [194.41 data/s]
Step 30200: loss = 2.00 (0.512 sec) [195.39 data/s]
Step 30300: loss = 1.92 (0.509 sec) [196.56 data/s]
Step 30400: loss = 1.85 (0.512 sec) [195.38 data/s]
Step 30500: loss = 1.93 (0.508 sec) [196.99 data/s]
Step 30600: loss = 1.87 (0.515 sec) [194.29 data/s]
Step 30700: loss = 2.04 (0.520 sec) [192.26 data/s]
Step 30800: loss = 1.96 (0.517 sec) [193.46 data/s]
Step 30900: loss = 1.92 (0.514 sec) [194.49 data/s]
Step 31000: loss = 1.75 (0.518 sec) [192.93 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 19166  Precision @ 1: 0.3833  Loss: 1.84
Validation Data Eval:
  Num examples: 10000  Num correct: 3848  Precision @ 1: 0.3848  Loss: 1.85
Step 31100: loss = 2.11 (0.520 sec) [192.36 data/s]
Step 31200: loss = 1.90 (0.514 sec) [194.38 data/s]
Step 31300: loss = 1.97 (0.519 sec) [192.70 data/s]
Step 31400: loss = 2.01 (0.517 sec) [193.35 data/s]
Step 31500: loss = 1.84 (0.513 sec) [194.80 data/s]
Step 31600: loss = 1.91 (0.514 sec) [194.71 data/s]
Step 31700: loss = 1.69 (0.516 sec) [193.83 data/s]
Step 31800: loss = 1.87 (0.520 sec) [192.23 data/s]
Step 31900: loss = 2.03 (0.518 sec) [192.97 data/s]
Step 32000: loss = 1.95 (0.515 sec) [194.20 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 19141  Precision @ 1: 0.3828  Loss: 1.84
Validation Data Eval:
  Num examples: 10000  Num correct: 3794  Precision @ 1: 0.3794  Loss: 1.86
Step 32100: loss = 1.84 (0.519 sec) [192.78 data/s]
Step 32200: loss = 1.92 (0.518 sec) [193.08 data/s]
Step 32300: loss = 2.11 (0.515 sec) [194.00 data/s]
Step 32400: loss = 1.89 (0.518 sec) [193.00 data/s]
Step 32500: loss = 1.94 (0.520 sec) [192.36 data/s]
Step 32600: loss = 1.72 (0.515 sec) [194.36 data/s]
Step 32700: loss = 1.84 (0.520 sec) [192.35 data/s]
Step 32800: loss = 2.09 (0.517 sec) [193.55 data/s]
Step 32900: loss = 2.10 (0.517 sec) [193.45 data/s]
Step 33000: loss = 1.81 (0.519 sec) [192.58 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 19045  Precision @ 1: 0.3809  Loss: 1.87
Validation Data Eval:
  Num examples: 10000  Num correct: 3773  Precision @ 1: 0.3773  Loss: 1.89
Step 33100: loss = 1.86 (0.511 sec) [195.71 data/s]
Step 33200: loss = 2.03 (0.519 sec) [192.71 data/s]
Step 33300: loss = 2.09 (0.514 sec) [194.38 data/s]
Step 33400: loss = 2.08 (0.517 sec) [193.50 data/s]
Step 33500: loss = 1.92 (0.516 sec) [193.62 data/s]
Step 33600: loss = 2.09 (0.515 sec) [194.20 data/s]
Step 33700: loss = 1.87 (0.511 sec) [195.71 data/s]
Step 33800: loss = 1.95 (0.519 sec) [192.81 data/s]
Step 33900: loss = 1.75 (0.512 sec) [195.37 data/s]
Step 34000: loss = 2.09 (0.514 sec) [194.45 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18981  Precision @ 1: 0.3796  Loss: 1.83
Validation Data Eval:
  Num examples: 10000  Num correct: 3734  Precision @ 1: 0.3734  Loss: 1.86
Step 34100: loss = 2.06 (0.518 sec) [193.10 data/s]
Step 34200: loss = 1.93 (0.520 sec) [192.16 data/s]
Step 34300: loss = 2.03 (0.518 sec) [192.87 data/s]
Step 34400: loss = 2.06 (0.519 sec) [192.75 data/s]
Step 34500: loss = 2.07 (0.519 sec) [192.61 data/s]
Step 34600: loss = 1.93 (0.518 sec) [193.21 data/s]
Step 34700: loss = 1.86 (0.520 sec) [192.40 data/s]
Step 34800: loss = 1.98 (0.516 sec) [193.82 data/s]
Step 34900: loss = 1.88 (0.516 sec) [193.82 data/s]
Step 35000: loss = 1.94 (0.515 sec) [194.13 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18597  Precision @ 1: 0.3719  Loss: 1.85
Validation Data Eval:
  Num examples: 10000  Num correct: 3743  Precision @ 1: 0.3743  Loss: 1.86
Step 35100: loss = 2.08 (0.513 sec) [194.78 data/s]
Step 35200: loss = 1.94 (0.515 sec) [194.13 data/s]
Step 35300: loss = 2.04 (0.515 sec) [194.11 data/s]
Step 35400: loss = 1.93 (0.516 sec) [193.70 data/s]
Step 35500: loss = 2.00 (0.519 sec) [192.74 data/s]
Step 35600: loss = 1.80 (0.515 sec) [194.08 data/s]
Step 35700: loss = 1.88 (0.517 sec) [193.44 data/s]
Step 35800: loss = 2.20 (0.516 sec) [193.74 data/s]
Step 35900: loss = 2.07 (0.514 sec) [194.60 data/s]
Step 36000: loss = 1.93 (0.515 sec) [194.14 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 19098  Precision @ 1: 0.3820  Loss: 1.86
Validation Data Eval:
  Num examples: 10000  Num correct: 3726  Precision @ 1: 0.3726  Loss: 1.89
Step 36100: loss = 1.97 (0.521 sec) [191.92 data/s]
Step 36200: loss = 2.03 (0.510 sec) [196.06 data/s]
Step 36300: loss = 2.13 (0.509 sec) [196.43 data/s]
Step 36400: loss = 1.98 (0.517 sec) [193.27 data/s]
Step 36500: loss = 1.95 (0.515 sec) [194.24 data/s]
Step 36600: loss = 1.82 (0.518 sec) [193.15 data/s]
Step 36700: loss = 1.84 (0.520 sec) [192.33 data/s]
Step 36800: loss = 1.83 (0.511 sec) [195.58 data/s]
Step 36900: loss = 1.95 (0.514 sec) [194.39 data/s]
Step 37000: loss = 1.90 (0.519 sec) [192.55 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18946  Precision @ 1: 0.3789  Loss: 1.84
Validation Data Eval:
  Num examples: 10000  Num correct: 3758  Precision @ 1: 0.3758  Loss: 1.85
Step 37100: loss = 1.93 (0.516 sec) [193.74 data/s]
Step 37200: loss = 1.99 (0.515 sec) [194.06 data/s]
Step 37300: loss = 2.04 (0.516 sec) [193.72 data/s]
Step 37400: loss = 2.06 (0.516 sec) [193.75 data/s]
Step 37500: loss = 1.81 (0.516 sec) [193.70 data/s]
Step 37600: loss = 2.10 (0.515 sec) [194.19 data/s]
Step 37700: loss = 1.99 (0.513 sec) [194.89 data/s]
Step 37800: loss = 1.96 (0.515 sec) [194.17 data/s]
Step 37900: loss = 2.01 (0.516 sec) [193.74 data/s]
Step 38000: loss = 2.10 (0.517 sec) [193.37 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18731  Precision @ 1: 0.3746  Loss: 1.86
Validation Data Eval:
  Num examples: 10000  Num correct: 3694  Precision @ 1: 0.3694  Loss: 1.89
Step 38100: loss = 2.07 (0.512 sec) [195.36 data/s]
Step 38200: loss = 1.91 (0.518 sec) [193.18 data/s]
Step 38300: loss = 2.07 (0.516 sec) [193.97 data/s]
Step 38400: loss = 2.06 (0.517 sec) [193.42 data/s]
Step 38500: loss = 1.89 (0.509 sec) [196.30 data/s]
Step 38600: loss = 1.98 (0.513 sec) [195.08 data/s]
Step 38700: loss = 1.95 (0.519 sec) [192.66 data/s]
Step 38800: loss = 1.77 (0.519 sec) [192.63 data/s]
Step 38900: loss = 1.80 (0.516 sec) [193.79 data/s]
Step 39000: loss = 2.03 (0.520 sec) [192.26 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 18405  Precision @ 1: 0.3681  Loss: 1.90
Validation Data Eval:
  Num examples: 10000  Num correct: 3640  Precision @ 1: 0.3640  Loss: 1.91
Step 39100: loss = 1.88 (0.516 sec) [193.76 data/s]
Step 39200: loss = 2.04 (0.513 sec) [194.75 data/s]
Step 39300: loss = 2.13 (0.518 sec) [192.94 data/s]
Step 39400: loss = 1.93 (0.514 sec) [194.39 data/s]
Step 39500: loss = 1.89 (0.511 sec) [195.62 data/s]
Step 39600: loss = 1.94 (0.518 sec) [193.19 data/s]
Step 39700: loss = 1.76 (0.518 sec) [193.08 data/s]
Step 39800: loss = 1.87 (0.517 sec) [193.60 data/s]
Step 39900: loss = 2.04 (0.514 sec) [194.39 data/s]
Step 40000: loss = 1.70 (0.510 sec) [196.10 data/s]
Training Data Eval:
  Num examples: 50000  Num correct: 19288  Precision @ 1: 0.3858  Loss: 1.82
Validation Data Eval:
  Num examples: 10000  Num correct: 3826  Precision @ 1: 0.3826  Loss: 1.84